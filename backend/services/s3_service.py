import boto3
from botocore.config import Config
import zipfile
import io
from typing import List, Dict
import os
from datetime import datetime
import hashlib

class S3Service:
    def __init__(self):
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è TWC Storage (S3-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ)
        endpoint_url = os.getenv('S3_ENDPOINT_URL')
        
        if endpoint_url:
            # –î–ª—è S3-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â (TWC Storage)
            self.s3 = boto3.client(
                's3',
                endpoint_url=endpoint_url,
                aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                region_name=os.getenv('AWS_REGION', 'ru-1'),
                config=Config(signature_version='s3v4')
            )
        else:
            # –î–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ AWS S3
            self.s3 = boto3.client(
                's3',
                aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                region_name=os.getenv('AWS_REGION', 'us-east-1'),
                config=Config(signature_version='s3v4')
            )
        
        self.bucket = os.getenv('S3_BUCKET_NAME', 'ai-database-images')
        self.endpoint_url = endpoint_url

    def _build_object_url(self, key: str, expires_in: int = 60 * 60 * 24 * 7) -> str:
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –æ–±—ä–µ–∫—Ç—É. –ü—ã—Ç–∞–µ—Ç—Å—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å presigned URL, –∏–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä—è–º–æ–π –ø—É—Ç—å."""
        try:
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º presigned URL, —á—Ç–æ–±—ã —Ä–∞–±–æ—Ç–∞–ª–æ –¥–∞–∂–µ –±–µ–∑ –ø—É–±–ª–∏—á–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            url = self.s3.generate_presigned_url(
                'get_object',
                Params={'Bucket': self.bucket, 'Key': key},
                ExpiresIn=expires_in
            )
            return url
        except Exception:
            # –§–æ–ª–±—ç–∫ –Ω–∞ –ø—Ä—è–º–æ–π URL
            if self.endpoint_url:
                return f"{self.endpoint_url}/{self.bucket}/{key}"
            else:
                return f"https://{self.bucket}.s3.amazonaws.com/{key}"

    async def upload_images_from_zip(self, zip_content: bytes) -> Dict[str, List[str]]:
        """–†–∞—Å–ø–∞–∫–æ–≤–∫–∞ ZIP –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ S3"""
        uploaded = []
        failed = []

        try:
            with zipfile.ZipFile(io.BytesIO(zip_content), 'r') as zip_ref:
                for file_info in zip_ref.filelist:
                    if file_info.filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                        try:
                            file_content = zip_ref.read(file_info.filename)
                            filename = os.path.basename(file_info.filename)
                            key = f"products/{filename}"

                            self.s3.put_object(
                                Bucket=self.bucket,
                                Key=key,
                                Body=file_content,
                                ContentType=self._get_content_type(filename)
                            )
                            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
                            url = self._build_object_url(key)
                            
                            uploaded.append({"filename": filename, "url": url})
                        except Exception as e:
                            failed.append({"filename": file_info.filename, "error": str(e)})
                    else:
                        failed.append({"filename": file_info.filename, "error": "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"})
        except Exception as e:
            return {"uploaded": [], "failed": [{"filename": "archive", "error": f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏: {e}"}]}

        return {"uploaded": uploaded, "failed": failed}

    async def upload_image(self, image_data: bytes, filename: str) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ S3"""
        key = f"products/{filename}"
        
        self.s3.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=image_data,
            ContentType=self._get_content_type(filename)
        )
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º URL –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (presigned)
        return self._build_object_url(key)

    async def list_folder_contents(self, folder_path: str = "/") -> Dict[str, List[Dict]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–∞–ø–∫–∏ (—ç–º—É–ª—è—Ü–∏—è —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –≤ S3)"""
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å
            prefix = folder_path.strip('/') + '/' if folder_path != '/' else ''
            print(f"DEBUG S3: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–∞–ø–∫–∏ '{folder_path}', prefix: '{prefix}'")
            print(f"DEBUG S3: S3 –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ - bucket: {self.bucket}, endpoint: {self.endpoint_url}")

            paginator = self.s3.get_paginator('list_objects_v2')
            folders_set = set()
            images = []

            for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix, Delimiter='/'):
                for prefix_info in page.get('CommonPrefixes', []):
                    folders_set.add(prefix_info['Prefix'])

                for obj in page.get('Contents', []):
                    if obj['Key'] == prefix:
                        continue
                    if not obj['Key'].lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp')):
                        continue
                    filename = obj['Key'].split('/')[-1]
                    url = self._build_object_url(obj['Key'])
                    images.append({
                        'id': obj['Key'],
                        'name': filename,
                        'path': '/' + obj['Key'],
                        'url': url,
                        'thumbnailUrl': url,
                        'size': obj.get('Size', 0),
                        'updatedAt': obj.get('LastModified').isoformat() if obj.get('LastModified') else datetime.now().isoformat()
                    })

            folders = []
            for folder_prefix in sorted(folders_set):
                folder_name = folder_prefix.rstrip('/').split('/')[-1]
                if not folder_name:
                    continue
                folders.append({
                    'id': folder_prefix,
                    'name': folder_name,
                    'path': '/' + folder_prefix.rstrip('/'),
                    'itemCount': await self._count_folder_items(folder_prefix),
                    'updatedAt': datetime.now().isoformat()
                })

            print(f"DEBUG S3: –ù–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫: {len(folders)}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")

            return {'folders': folders, 'images': images}
            
        except Exception as e:
            print(f"DEBUG S3: –û–®–ò–ë–ö–ê –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–∞–ø–∫–∏ {folder_path}: {e}")
            print(f"DEBUG S3: –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
            import traceback
            traceback.print_exc()
            return {'folders': [], 'images': []}

    async def create_folder(self, parent_path: str, folder_name: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ (–∑–∞–≥—Ä—É–∑–∫–∞ –ø—É—Å—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º /)"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å
        parent = parent_path.strip('/') + '/' if parent_path != '/' else ''
        folder_key = f"{parent}{folder_name}/"
        
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –æ–±—ä–µ–∫—Ç, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π –ø–∞–ø–∫—É
        self.s3.put_object(
            Bucket=self.bucket,
            Key=folder_key,
            Body=b'',
            ContentType='application/x-directory'
        )
        
        return folder_key

    async def delete_folder(self, folder_path: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –ø–∞–ø–∫–∏ –∏ –≤—Å–µ–≥–æ –µ—ë —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        prefix = folder_path.strip('/') + '/'

        paginator = self.s3.get_paginator('list_objects_v2')
        batch = []
        for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
            for obj in page.get('Contents', []):
                batch.append({'Key': obj['Key']})
                if len(batch) == 1000:
                    self.s3.delete_objects(Bucket=self.bucket, Delete={'Objects': batch})
                    batch = []
        if batch:
            self.s3.delete_objects(Bucket=self.bucket, Delete={'Objects': batch})

    async def rename_folder(self, old_path: str, new_name: str) -> str:
        """–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ (–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–æ–≤—ã–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–º)"""
        old_prefix = old_path.strip('/') + '/'
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã–π –ø—Ä–µ—Ñ–∏–∫—Å
        path_parts = old_path.strip('/').split('/')
        path_parts[-1] = new_name
        new_prefix = '/'.join(path_parts) + '/'

        paginator = self.s3.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=self.bucket, Prefix=old_prefix):
            for obj in page.get('Contents', []):
                old_key = obj['Key']
                new_key = old_key.replace(old_prefix, new_prefix, 1)
                self.s3.copy_object(
                    Bucket=self.bucket,
                    CopySource={'Bucket': self.bucket, 'Key': old_key},
                    Key=new_key
                )
                self.s3.delete_object(Bucket=self.bucket, Key=old_key)
        
        return new_prefix

    async def check_duplicate_image(self, image_data: bytes, hash_cache: Dict[str, Dict] = None) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Ö–µ—à—É —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º MD5 —Ö–µ—à –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_hash = hashlib.md5(image_data).hexdigest()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω (–¥–ª—è –º–∞—Å—Å–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π)
            if hash_cache is not None:
                if image_hash in hash_cache:
                    existing = hash_cache[image_hash]
                    return {
                        'is_duplicate': True,
                        'existing_file': existing
                    }
                return {'is_duplicate': False}
            
            # –ò—â–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Ç–∞–∫–∏–º –∂–µ —Ö–µ—à–µ–º –≤ S3 (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫)
            paginator = self.s3.get_paginator('list_objects_v2')
            
            for page in paginator.paginate(Bucket=self.bucket):
                for obj in page.get('Contents', []):
                    if not obj['Key'].lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp', '.bmp', '.tiff')):
                        continue
                        
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
                        response = self.s3.head_object(Bucket=self.bucket, Key=obj['Key'])
                        metadata = response.get('Metadata', {})
                        
                        # –ï—Å–ª–∏ —Ö–µ—à –µ—Å—Ç—å –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç - —ç—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç
                        if metadata.get('image-hash') == image_hash:
                            return {
                                'is_duplicate': True,
                                'existing_file': {
                                    'key': obj['Key'],
                                    'url': self._build_object_url(obj['Key']),
                                    'size': obj['Size'],
                                    'last_modified': obj['LastModified'].isoformat()
                                }
                            }
                            
                    except Exception as e:
                        # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–∏—Å–∫
                        continue
                        
            return {'is_duplicate': False}
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞: {e}")
            return {'is_duplicate': False}

    async def _build_hash_cache(self) -> Dict[str, Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–µ—à–∞ —Ö–µ—à–µ–π –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ S3 –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        hash_cache = {}
        try:
            paginator = self.s3.get_paginator('list_objects_v2')
            
            for page in paginator.paginate(Bucket=self.bucket):
                for obj in page.get('Contents', []):
                    if not obj['Key'].lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp', '.bmp', '.tiff')):
                        continue
                        
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç–∞
                        response = self.s3.head_object(Bucket=self.bucket, Key=obj['Key'])
                        metadata = response.get('Metadata', {})
                        image_hash = metadata.get('image-hash')
                        
                        if image_hash:
                            hash_cache[image_hash] = {
                                'key': obj['Key'],
                                'url': self._build_object_url(obj['Key']),
                                'size': obj['Size'],
                                'last_modified': obj['LastModified'].isoformat()
                            }
                    except Exception:
                        continue
                        
            print(f"–°–æ–∑–¥–∞–Ω –∫–µ—à —Ö–µ—à–µ–π –¥–ª—è {len(hash_cache)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return hash_cache
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–µ—à–∞ —Ö–µ—à–µ–π: {e}")
            return {}

    async def upload_image_to_path(self, image_data: bytes, file_path: str, check_duplicate: bool = True) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        key = file_path.lstrip('/')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –æ–ø—Ü–∏—è
        if check_duplicate:
            duplicate_check = await self.check_duplicate_image(image_data)
            if duplicate_check['is_duplicate']:
                existing = duplicate_check['existing_file']
                # –ï—Å–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç –Ω–∞–π–¥–µ–Ω, –∑–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª –Ω–æ–≤—ã–º
                await self.delete_image_by_key(existing['key'])
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        image_hash = hashlib.md5(image_data).hexdigest()
        
        self.s3.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=image_data,
            ContentType=self._get_content_type(file_path),
            Metadata={'image-hash': image_hash},
            ACL='public-read'
        )
        
        url = self._build_object_url(key)
        
        return {
            'url': url,
            'replaced_duplicate': duplicate_check.get('is_duplicate', False) if check_duplicate else False,
            'replaced_file': duplicate_check.get('existing_file') if check_duplicate and duplicate_check.get('is_duplicate') else None
        }

    async def upload_zip_to_folder(self, zip_content: bytes, folder_path: str) -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ ZIP: –ª–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ ‚Üí S3 –±–∞—Ç—á–∏"""
        import asyncio
        import time
        import tempfile
        import os
        import shutil
        from concurrent.futures import ThreadPoolExecutor
        
        uploaded = []
        failed = []
        start_time = time.time()
        temp_dir = None

        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∞—Ä—Ö–∏–≤–∞ (–º–∞–∫—Å 2GB)
            if len(zip_content) > 2 * 1024 * 1024 * 1024:
                return {"uploaded": [], "failed": [{"filename": "archive", "error": "–ê—Ä—Ö–∏–≤ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å–∏–º—É–º 2GB)"}]}

            base_prefix = folder_path.strip('/') + '/' if folder_path != '/' else ''
            
            print(f"üì¶ –ù–∞—á–∏–Ω–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É ZIP –∞—Ä—Ö–∏–≤–∞ —Ä–∞–∑–º–µ—Ä–æ–º {len(zip_content) / (1024*1024):.1f} MB")
            
            # –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤
            print("ÔøΩÔ∏è  –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏...")
            temp_dir = tempfile.mkdtemp(prefix='zip_upload_')
            
            print("üìÇ –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤ –ª–æ–∫–∞–ª—å–Ω–æ...")
            extract_start = time.time()
            
            with zipfile.ZipFile(io.BytesIO(zip_content), 'r') as zip_ref:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                image_files = []
                
                for file_info in zip_ref.infolist():
                    name = file_info.filename
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º zip slip
                    norm = os.path.normpath(name)
                    if norm.startswith('..') or norm.startswith('/'):
                        failed.append({"filename": name, "error": "Invalid path in zip"})
                        continue

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥–∏
                    if name.endswith('/'):
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
                    if not name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp', '.bmp', '.tiff')):
                        failed.append({"filename": name, "error": "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞"})
                        continue

                    try:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–π–ª –≤ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
                        safe_path = os.path.join(temp_dir, os.path.basename(norm))
                        with open(safe_path, 'wb') as f:
                            f.write(zip_ref.read(file_info))
                        
                        image_files.append({
                            'original_name': norm,
                            'local_path': safe_path,
                            's3_key': f"{base_prefix}{norm}"
                        })
                    except Exception as e:
                        failed.append({"filename": name, "error": f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}"})

            extract_time = time.time() - extract_start
            total_files = len(image_files)
            print(f"‚úÖ –†–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ {total_files} —Ñ–∞–π–ª–æ–≤ –∑–∞ {extract_time:.2f}—Å")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤
            if total_files > 10000:
                return {"uploaded": [], "failed": [{"filename": "archive", "error": f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ñ–∞–π–ª–æ–≤ –≤ –∞—Ä—Ö–∏–≤–µ (–º–∞–∫—Å–∏–º—É–º 10000, –Ω–∞–π–¥–µ–Ω–æ {total_files})"}]}

            if total_files == 0:
                return {"uploaded": [], "failed": [], "message": "–í –∞—Ä—Ö–∏–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏"}

            # –≠—Ç–∞–ø 2: –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–∞–ø–∫–µ S3
            print(f"üîç –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤ S3 –ø–∞–ø–∫–µ '{folder_path}'...")
            s3_list_start = time.time()
            
            existing_files = {}
            try:
                response = self.s3.list_objects_v2(
                    Bucket=self.bucket,
                    Prefix=base_prefix,
                    MaxKeys=10000
                )
                
                if 'Contents' in response:
                    for obj in response['Contents']:
                        key = obj['Key']
                        filename = key.replace(base_prefix, '')
                        if filename:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞–ø–∫–∏
                            existing_files[filename] = {
                                'key': key,
                                'size': obj['Size'],
                                'last_modified': obj['LastModified'].isoformat()
                            }
                            
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ S3 —Ñ–∞–π–ª–æ–≤: {e}")
            
            s3_list_time = time.time() - s3_list_start
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(existing_files)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤ S3 –∑–∞ {s3_list_time:.2f}—Å")

            # –≠—Ç–∞–ø 3: –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —á—Ç–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å
            print("üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å/–∑–∞–º–µ–Ω–∏—Ç—å...")
            
            files_to_upload = []  # –ù–æ–≤—ã–µ —Ñ–∞–π–ª—ã
            files_to_replace = []  # –§–∞–π–ª—ã –¥–ª—è –∑–∞–º–µ–Ω—ã
            replaced_count = 0
            
            for file_info in image_files:
                filename = os.path.basename(file_info['original_name'])
                
                if filename in existing_files:
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
                    local_size = os.path.getsize(file_info['local_path'])
                    s3_size = existing_files[filename]['size']
                    
                    if local_size != s3_size:
                        files_to_replace.append(file_info)
                        replaced_count += 1
                        print(f"üîÑ –ó–∞–º–µ–Ω–∏–º {filename} (—Ä–∞–∑–º–µ—Ä –∏–∑–º–µ–Ω–∏–ª—Å—è: {s3_size} ‚Üí {local_size})")
                    else:
                        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {filename} (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)")
                else:
                    files_to_upload.append(file_info)
            
            all_upload_files = files_to_upload + files_to_replace
            upload_count = len(all_upload_files)
            
            print(f"üìà –ö –∑–∞–≥—Ä—É–∑–∫–µ: {len(files_to_upload)} –Ω–æ–≤—ã—Ö + {len(files_to_replace)} –∑–∞–º–µ–Ω = {upload_count} —Ñ–∞–π–ª–æ–≤")
            
            if upload_count == 0:
                return {
                    "uploaded": [],
                    "failed": failed,
                    "message": "–í—Å–µ —Ñ–∞–π–ª—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ S3, –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è",
                    "replaced_duplicates": 0,
                    "total_processed": total_files,
                    "processing_time": round(time.time() - start_time, 1)
                }

            # –≠—Ç–∞–ø 4: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ S3 —Å –æ—Ç—á–µ—Ç–æ–º –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É {upload_count} —Ñ–∞–π–ª–æ–≤ –≤ S3...")
            
            uploaded_count = 0
            failed_count = 0
            last_progress_report = 0
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è –ª—É—á—à–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
            semaphore = asyncio.Semaphore(75)  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 50 –¥–æ 75
            
            async def upload_single_file(file_info):
                nonlocal uploaded_count, failed_count, last_progress_report
                
                async with semaphore:
                    try:
                        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å –¥–∏—Å–∫–∞
                        with open(file_info['local_path'], 'rb') as f:
                            file_content = f.read()
                        
                        content_type = self._get_content_type(file_info['original_name'])
                        image_hash = hashlib.md5(file_content).hexdigest()
                        
                        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ S3
                        loop = asyncio.get_event_loop()
                        await loop.run_in_executor(
                            None,
                            lambda: self.s3.put_object(
                                Bucket=self.bucket,
                                Key=file_info['s3_key'],
                                Body=file_content,
                                ContentType=content_type,
                                Metadata={'image-hash': image_hash}
                            )
                        )
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏
                        uploaded_count += 1
                        
                        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5% –∏–ª–∏ –∫–∞–∂–¥—ã–µ 50 —Ñ–∞–π–ª–æ–≤
                        progress_percent = (uploaded_count + failed_count) / upload_count * 100
                        if progress_percent - last_progress_report >= 5 or (uploaded_count + failed_count) % 50 == 0:
                            elapsed = time.time() - start_time
                            rate = (uploaded_count + failed_count) / elapsed if elapsed > 0 else 0
                            eta = (upload_count - uploaded_count - failed_count) / rate if rate > 0 else 0
                            print(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {uploaded_count + failed_count}/{upload_count} ({progress_percent:.1f}%) | "
                                  f"–°–∫–æ—Ä–æ—Å—Ç—å: {rate:.1f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫ | ETA: {eta:.0f}—Å–µ–∫")
                            last_progress_report = progress_percent
                        
                        url = self._build_object_url(file_info['s3_key'])
                        return {
                            "filename": file_info['original_name'],
                            "url": url,
                            "size": len(file_content),
                            "status": "success"
                        }
                        
                    except Exception as e:
                        failed_count += 1
                        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_info['original_name']}: {e}")
                        return {
                            "filename": file_info['original_name'],
                            "error": str(e),
                            "status": "failed"
                        }

            # –°–æ–∑–¥–∞–µ–º –∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [upload_single_file(file_info) for file_info in all_upload_files]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in results:
                if isinstance(result, Exception):
                    failed.append({"filename": "unknown", "error": str(result)})
                elif result.get("status") == "success":
                    uploaded.append({
                        "filename": result["filename"],
                        "url": result["url"],
                        "size": result["size"]
                    })
                else:
                    failed.append({
                        "filename": result["filename"],
                        "error": result["error"]
                    })

            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_time = time.time() - start_time
            success_count = len(uploaded)
            failed_count = len(failed)
            
            print(f"üéâ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            print(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {success_count}")
            print(f"   üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–º–µ–Ω–µ–Ω–æ: {replaced_count}")
            print(f"   ‚ùå –û—à–∏–±–æ–∫: {failed_count}")
            print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å")
            print(f"   üöÄ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_files / total_time:.1f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫")
            
            message = f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {success_count} –∏–∑ {total_files} —Ñ–∞–π–ª–æ–≤ –∑–∞ {total_time:.1f}—Å"
            if replaced_count > 0:
                message += f" ({replaced_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–º–µ–Ω–µ–Ω–æ)"

            return {
                "uploaded": uploaded,
                "failed": failed,
                "message": message,
                "replaced_duplicates": replaced_count,
                "total_processed": total_files,
                "processing_time": round(total_time, 1)
            }

        except zipfile.BadZipFile:
            return {"uploaded": [], "failed": [{"filename": "archive", "error": "–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π ZIP –∞—Ä—Ö–∏–≤"}]}
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ ZIP: {e}")
            return {"uploaded": [], "failed": [{"filename": "archive", "error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—Ä—Ö–∏–≤–∞: {e}"}]}
        finally:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
            if temp_dir and os.path.exists(temp_dir):
                try:
                    shutil.rmtree(temp_dir)
                    print(f"üßπ –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ {temp_dir} –æ—á–∏—â–µ–Ω–∞")
                except Exception as e:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É: {e}")

    async def delete_image_by_key(self, key: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –∫–ª—é—á—É S3"""
        self.s3.delete_object(Bucket=self.bucket, Key=key)

    async def delete_image_by_path(self, image_path: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –ø—É—Ç–∏"""
        key = image_path.lstrip('/')
        await self.delete_image_by_key(key)

    def _is_image_path(self, path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é, —á—Ç–æ –ø—É—Ç—å —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        p = path.lower()
        return any(p.endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.bmp', '.tiff'])

    async def delete_images_batch(self, image_paths: list[str]) -> dict:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–∞—Ç—á–∞–º–∏ —á–µ—Ä–µ–∑ S3 DeleteObjects (–¥–æ 1000 –∑–∞ —Ä–∞–∑).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç { deleted: int, errors: [keys...] } –≥–¥–µ keys ‚Äî S3-–∫–ª—é—á–∏ –Ω–µ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤.
        """
        keys = [p.lstrip('/') for p in image_paths]
        total_deleted = 0
        error_keys: list[str] = []

        for i in range(0, len(keys), 1000):
            chunk = keys[i:i + 1000]
            try:
                resp = self.s3.delete_objects(
                    Bucket=self.bucket,
                    Delete={'Objects': [{'Key': k} for k in chunk]}
                )
                total_deleted += len(resp.get('Deleted', []) or [])
                for err in resp.get('Errors', []) or []:
                    if err.get('Key'):
                        error_keys.append(err['Key'])
            except Exception:
                # –ï—Å–ª–∏ —É–¥–∞–ª–µ–Ω–∏–µ —á–∞–Ω–∫–∞ —É–ø–∞–ª–æ, —Å—á–∏—Ç–∞–µ–º –≤—Å–µ –∫–ª—é—á–∏ –∏–∑ –Ω–µ–≥–æ –æ—à–∏–±–æ—á–Ω—ã–º–∏
                error_keys.extend(chunk)

        return {"deleted": total_deleted, "errors": error_keys}

    async def rename_image(self, old_path: str, new_name: str) -> str:
        """–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        old_key = old_path.lstrip('/')
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã–π –∫–ª—é—á
        path_parts = old_key.split('/')
        path_parts[-1] = new_name
        new_key = '/'.join(path_parts)
        
        # –ö–æ–ø–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç —Å –Ω–æ–≤—ã–º –∏–º–µ–Ω–µ–º
        self.s3.copy_object(
            Bucket=self.bucket,
            CopySource={'Bucket': self.bucket, 'Key': old_key},
            Key=new_key
        )
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –æ–±—ä–µ–∫—Ç
        self.s3.delete_object(Bucket=self.bucket, Key=old_key)
        
        return new_key

    async def move_folder(self, old_path: str, new_path: str):
        """–ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –ø–∞–ø–∫–∏"""
        old_prefix = old_path.strip('/') + '/'
        new_prefix = new_path.strip('/') + '/'

        paginator = self.s3.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=self.bucket, Prefix=old_prefix):
            for obj in page.get('Contents', []):
                old_key = obj['Key']
                new_key = old_key.replace(old_prefix, new_prefix, 1)
                self.s3.copy_object(
                    Bucket=self.bucket,
                    CopySource={'Bucket': self.bucket, 'Key': old_key},
                    Key=new_key
                )
                self.s3.delete_object(Bucket=self.bucket, Key=old_key)

    async def move_image(self, old_path: str, new_path: str):
        """–ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        old_key = old_path.lstrip('/')
        new_key = new_path.lstrip('/')
        
        # –ö–æ–ø–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç
        self.s3.copy_object(
            Bucket=self.bucket,
            CopySource={'Bucket': self.bucket, 'Key': old_key},
            Key=new_key
        )
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –æ–±—ä–µ–∫—Ç
        self.s3.delete_object(Bucket=self.bucket, Key=old_key)

    async def move_images_concurrent(self, pairs: list[tuple[str, str]], max_workers: int = 20) -> dict:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        pairs: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (old_path, new_path)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: { moved: int, errors: [{src, dst, error}] }
        """
        import asyncio

        loop = asyncio.get_event_loop()
        semaphore = asyncio.Semaphore(max_workers)

        async def move_one(src: str, dst: str):
            async with semaphore:
                old_key = src.lstrip('/')
                new_key = dst.lstrip('/')
                def op():
                    # copy + delete
                    self.s3.copy_object(
                        Bucket=self.bucket,
                        CopySource={'Bucket': self.bucket, 'Key': old_key},
                        Key=new_key
                    )
                    self.s3.delete_object(Bucket=self.bucket, Key=old_key)
                await loop.run_in_executor(None, op)

        moved = 0
        errors: list[dict] = []
        tasks = []
        for src, dst in pairs:
            async def wrapper(s=src, d=dst):
                nonlocal moved, errors
                try:
                    await move_one(s, d)
                    moved += 1
                except Exception as e:
                    errors.append({"src": s, "dst": d, "error": str(e)})
            tasks.append(wrapper())

        await asyncio.gather(*tasks, return_exceptions=False)
        return {"moved": moved, "errors": errors}

    async def search_images(self, query: str, folder_path: str = "/") -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        try:
            prefix = folder_path.strip('/') + '/' if folder_path != '/' else ''

            results = []
            max_results = 500
            paginator = self.s3.get_paginator('list_objects_v2')
            q = query.lower()
            for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):
                for obj in page.get('Contents', []):
                    key = obj['Key']
                    if not key.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp')):
                        continue
                    if q and q not in key.lower():
                        continue
                    filename = key.split('/')[-1]
                    url = self._build_object_url(key)
                    results.append({
                        'id': key,
                        'name': filename,
                        'path': '/' + key,
                        'url': url,
                        'size': obj.get('Size', 0),
                        'updatedAt': obj.get('LastModified').isoformat() if obj.get('LastModified') else datetime.now().isoformat()
                    })
                    if len(results) >= max_results:
                        return results

            return results
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            return []

    async def _count_folder_items(self, prefix: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –ø–∞–ø–∫–µ (—Ç–æ–ª—å–∫–æ –ø—Ä—è–º—ã–µ –¥–µ—Ç–∏: —Ñ–∞–π–ª—ã –∏ –ø–æ–¥–ø–∞–ø–∫–∏)"""
        try:
            paginator = self.s3.get_paginator('list_objects_v2')
            total = 0
            for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix, Delimiter='/'):
                total += len(page.get('Contents', []))
                total += len(page.get('CommonPrefixes', []))
            return total
        except Exception as e:
            print(f"DEBUG S3: –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á–µ—Ç–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è {prefix}: {e}")
            return 0

    def _get_content_type(self, filename: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Content-Type –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é"""
        ext = filename.lower().split('.')[-1]
        types = {
            'png': 'image/png',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'gif': 'image/gif',
            'webp': 'image/webp'
        }
        return types.get(ext, 'application/octet-stream')

    async def delete_image(self, url: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ S3"""
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ TWC Storage, —Ç–∞–∫ –∏ AWS S3 URL
            if self.endpoint_url and self.endpoint_url in url:
                key = url.split(f"{self.endpoint_url}/{self.bucket}/")[-1]
            else:
                key = url.split(f"https://{self.bucket}.s3.amazonaws.com/")[-1]
            
            self.s3.delete_object(Bucket=self.bucket, Key=key)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {url}: {e}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
s3_service = S3Service()
